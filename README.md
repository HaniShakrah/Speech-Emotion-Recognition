# Speech Emotion Recognition

The proposed dataset is RAVDESS dataset which contains 1,440 audio-only files of speech
recorded from 24 professional voice actors (12 female, 12 male), vocalizing two
lexically-matched statements in a neutral North American accent. Speech emotions include calm,
happy, sad, angry, fearful, surprised, and disgust expressions. Each expression is produced at two
levels of emotional intensity (normal, strong), with an additional neutral expression.

Audio classification and Speech Emotion Recognition (S.E.R) play pivotal roles in audio signal
processing, with applications spanning music genre classification, speech recognition, and
emotional analysis. Leveraging deep learning, particularly CNNs and RNNs, enhances
performance in these tasks. Input representation, whether MFCCs, mel-spectrograms, or raw
audio, significantly influences model performance. In this project, we developed a CNN that took in mel-spectrogram images as input. 
